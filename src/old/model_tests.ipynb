{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.grace/eec42/BirdDetector/src\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/src/data_preprocessing\n"
     ]
    }
   ],
   "source": [
    "#import ultralytics\n",
    "#ultralytics.checks()\n",
    "#from ultralytics import YOLO\n",
    "\n",
    "#import dayolo\n",
    "#from dayolo import YOLO\n",
    "\n",
    "import yolo\n",
    "from yolo import YOLO\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "import random\n",
    "#from ultralytics.utils.plotting import plot_labels\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/data_preprocessing'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "#import visualization_utils as visutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0) # Set to your desired GPU number\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING INSTANCE DOMAIN CLASSIFIER MODEL\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 54        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      "YOLOv8m summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Transferred 469/475 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "# Create a new YOLO model from scratch\n",
    "#model = YOLO('yolov8n.yaml')\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training) - for now, we keep the small version\n",
    "#pretrained_model_name = 'pfeifer_penguins_poland_10percentbckgd_yolov8m_120epoch'\n",
    "#model_path = 'runs/detect/' + pretrained_model_name + '/weights/best.pt'\n",
    "#model = YOLO('yolov8m.pt', task='da_detect')\n",
    "#model = YOLO(\"yolov8.yaml\").load(\"yolov8m.pt\")\n",
    "#model = YOLO('yolov8m.pt', task='da_detect')\n",
    "#model = YOLO('yolov8m.pt', task='detect')\n",
    "#model = YOLO(\"yolov8m.yaml\", task='detect').load(\"yolov8m.pt\")\n",
    "model = YOLO(\"yolov8m.yaml\", task='detect', subtask='instanceDC').load(\"yolov8m.pt\")\n",
    "\n",
    "\n",
    "#model.load_weights('yolov8m.pt')\n",
    "\n",
    "\n",
    "#PRETRAINED_MODEL_NAME = 'pfeifer_penguins_poland_10percentbckgd_yolov8m_120epoch'\n",
    "#PRETRAINED_MODEL_PATH = 'src/model/runs/detect/' + PRETRAINED_MODEL_NAME + '/weights/best.pt'\n",
    "\n",
    "#MODEL_NAME = 'pfeifer_penguins_poland_palmyra_10percent_bckgd_yolov8m_120epochs'\n",
    "#TASK = 'detect' # Choose between: 'deepcoral_detect' 'detect'\n",
    "#model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect\n",
      "instanceDC\n"
     ]
    }
   ],
   "source": [
    "print(model.task)\n",
    "print(model.subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 22\n",
      "Detect(\n",
      "  (cv2): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (cv3): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (dfl): DFL(\n",
      "    (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.model.model\n",
    "for i, a in enumerate(model.model.model):\n",
    "    if i == 22:\n",
    "        print(\"block\", i)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.14 available 😃 Update with 'pip install -U ultralytics'\n",
      "New https://pypi.org/project/ultralytics/8.1.14 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.228 🚀 Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "Ultralytics YOLOv8.0.228 🚀 Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=data.yaml, epochs=1, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=TEST_features_loss23, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, source_name=global_birds_palmyra, val=False, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, dc=1.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=90, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/TEST_features_loss23\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=data.yaml, epochs=1, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=TEST_features_loss23, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, source_name=global_birds_palmyra, val=False, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, dc=1.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=90, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/TEST_features_loss23\n",
      "INITIALIZING INSTANCE DOMAIN CLASSIFIER MODEL\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 54        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      " 54        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      "YOLOv8m summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "YOLOv8m summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "Transferred 475/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/train/labels.cache... 1722 images, 182 backgrounds, 0 corrupt: 100%|██████████| 1904/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/val/labels.cache... 248 images, 19 backgrounds, 0 corrupt: 100%|██████████| 267/267 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/TEST_features_loss23/labels.jpg... \n",
      "Plotting labels to runs/detect/TEST_features_loss23/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "1 epochs...\n",
      "1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss    da_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss    da_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pooling torch.Size([8, 1, 5, 5])\n",
      "torch.Size([8, 1, 1, 1])\n",
      "torch.Size([8, 1])\n",
      "after fc1 torch.Size([8, 2])\n",
      "selected_features torch.Size([33600, 576, 10, 10])\n",
      "before pooling torch.Size([33600, 1, 10, 10])\n",
      "torch.Size([33600, 1, 1, 1])\n",
      "torch.Size([33600, 1])\n",
      "after fc1 torch.Size([33600, 2])\n",
      "instances_domainpreds torch.Size([33600, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.21 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.28 GiB is free. Including non-PyTorch memory, this process has 25.46 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, and 4.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TRAIN the model on our dataset (data.yml config file) \u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST_features_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#imgsz=480,  # we are trying with several img size so we do not precise the size -> will automatically resize all images to 640x640\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m   \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#32,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#cos_lr=True,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#dropout=0.3,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#optimizer='Adam',\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m   \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m   \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#dc = [0.5, 0.5, 0.5],\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#lr0=0.001,\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#lrf=0.0001,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfliplr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflipud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# augmentation parameters\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m   \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/engine/model.py:340\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/engine/trainer.py:190\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/models/yolo/detect/train.py:2801\u001b[0m, in \u001b[0;36mInstanceDomainClassifierTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;66;03m#print(\"losses\", self.loss)\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;66;03m#print(self.scaler.scale(self.loss).backward())\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \n\u001b[1;32m   2804\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/.conda/envs/pdm/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pdm/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.21 GiB. GPU 0 has a total capacty of 31.74 GiB of which 6.28 GiB is free. Including non-PyTorch memory, this process has 25.46 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, and 4.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# TRAIN the model on our dataset (data.yml config file) \n",
    "\n",
    "model_name = 'TEST_features_loss'\n",
    "\n",
    "results = model.train(\n",
    "   data='data.yaml',\n",
    "   #imgsz=480,  # we are trying with several img size so we do not precise the size -> will automatically resize all images to 640x640\n",
    "   epochs=1,\n",
    "   batch=8, #32,\n",
    "   #cos_lr=True,\n",
    "   #dropout=0.3,\n",
    "   #optimizer='Adam',\n",
    "   patience=10,\n",
    "   device=0,\n",
    "   verbose=True,\n",
    "   val = False,\n",
    "   #dc = [0.5, 0.5, 0.5],\n",
    "   #lr0=0.001,\n",
    "   #lrf=0.0001,\n",
    "   degrees=90, fliplr=0.5, flipud=0.5, scale=0.5, # augmentation parameters\n",
    "   name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "3\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "3\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "[tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'list_of_tensors' is your list of tensors and 'B' is your tensor containing 0 and 1 values\n",
    "list_of_tensors = [torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]),\n",
    "                   torch.tensor([[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]),\n",
    "                   torch.tensor([[25, 26, 27, 28], [29, 30, 31, 32], [33, 34, 35, 36]])]\n",
    "\n",
    "B = torch.tensor([0, 1, 0])\n",
    "\n",
    "# Get the size of the list of tensors\n",
    "N = list_of_tensors[0].size(0)\n",
    "\n",
    "output = []\n",
    "for i in range(len(list_of_tensors)):\n",
    "    print(list_of_tensors[i].size()[0])\n",
    "    for j in range(list_of_tensors[i].size()[0]):\n",
    "        print(B[i])\n",
    "        output.append(B[i])\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mB\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx = torch.Tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 3., 3., 4., 5., 6., 7.])\n",
    "annotations_mask = [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]\n",
    "\n",
    "new_batch_idx = batch_idx[annotations_mask]\n",
    "new_batch_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect(\n",
      "  (cv2): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (cv3): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (dfl): DFL(\n",
      "    (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "b = new_batch_idx.unique(return_inverse=True)\n",
    "b[1]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'DAN_pe_palmyra_10percentbkgd_dcgain1.5'\n",
    "model_path = '/home/eec42/BirdDetector/runs/detect/train5/weights/last.pt'\n",
    "model = YOLO(model_path, task='detect', subtask='domainclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0: 640x640 88 birds, 1: 640x640 (no detections), 2: 640x640 1 bird, 3: 640x640 (no detections), 4: 640x640 3 birds, 5: 640x640 (no detections), 6: 640x640 1 bird, 7: 640x640 15 birds, 8: 640x640 1 bird, 9: 640x640 50 birds, 10: 640x640 7 birds, 11: 640x640 2 birds, 12: 640x640 (no detections), 13: 640x640 1 bird, 14: 640x640 2 birds, 15: 640x640 11 birds, 16: 640x640 84 birds, 17: 640x640 84 birds, 18: 640x640 3 birds, 19: 640x640 12 birds, 20: 640x640 3 birds, 21: 640x640 2 birds, 22: 640x640 1 bird, 23: 640x640 1 bird, 24: 640x640 1 bird, 25: 640x640 (no detections), 26: 640x640 1 bird, 27: 640x640 2 birds, 28: 640x640 (no detections), 29: 640x640 1 bird, 30: 640x640 1 bird, 31: 640x640 2 birds, 32: 640x640 (no detections), 33: 640x640 8 birds, 34: 640x640 (no detections), 35: 640x640 1 bird, 36: 640x640 3 birds, 37: 640x640 3 birds, 38: 640x640 48 birds, 39: 640x640 1 bird, 464.5ms\n",
      "0: 640x640 88 birds, 1: 640x640 (no detections), 2: 640x640 1 bird, 3: 640x640 (no detections), 4: 640x640 3 birds, 5: 640x640 (no detections), 6: 640x640 1 bird, 7: 640x640 15 birds, 8: 640x640 1 bird, 9: 640x640 50 birds, 10: 640x640 7 birds, 11: 640x640 2 birds, 12: 640x640 (no detections), 13: 640x640 1 bird, 14: 640x640 2 birds, 15: 640x640 11 birds, 16: 640x640 84 birds, 17: 640x640 84 birds, 18: 640x640 3 birds, 19: 640x640 12 birds, 20: 640x640 3 birds, 21: 640x640 2 birds, 22: 640x640 1 bird, 23: 640x640 1 bird, 24: 640x640 1 bird, 25: 640x640 (no detections), 26: 640x640 1 bird, 27: 640x640 2 birds, 28: 640x640 (no detections), 29: 640x640 1 bird, 30: 640x640 1 bird, 31: 640x640 2 birds, 32: 640x640 (no detections), 33: 640x640 8 birds, 34: 640x640 (no detections), 35: 640x640 1 bird, 36: 640x640 3 birds, 37: 640x640 3 birds, 38: 640x640 48 birds, 39: 640x640 1 bird, 464.5ms\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_img = []\n",
    "img_path = '/gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/test/'\n",
    "selected_img.extend(random.choices(os.listdir(img_path + '/images/'), k=40))\n",
    "\n",
    "# Predict results for randomly selected images\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = 0.1, \n",
    "        iou = 0.1,\n",
    "        show = False,\n",
    "        save = False\n",
    "    )\n",
    "\n",
    "features = torch.load('/home/eec42/BirdDetector/runs/detect/DAN_pe_palmyra_10percentbkgd_dcgain1.5/features.pt')\n",
    "\n",
    "C,H,W = features[0].size()\n",
    "feats = []\n",
    "\n",
    "for i in range(len(features)):\n",
    "    reshaped_tensor = features[0].reshape(C, -1)\n",
    "    concatenated_tensor = torch.cat([reshaped_tensor[i] for i in range(C)], dim=0).cpu()\n",
    "\n",
    "    feats.append(concatenated_tensor)\n",
    "\n",
    "feats_target = torch.stack(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.1237e-05, -2.9863e-13],\n",
       "       [ 3.1237e-05, -3.9286e-13],\n",
       "       [ 3.1237e-05, -4.2712e-13],\n",
       "       [ 3.1237e-05,   4.332e-13],\n",
       "       [ 3.1237e-05,  2.0543e-13],\n",
       "       [ 3.1237e-05,  -6.413e-14],\n",
       "       [ 3.1237e-05, -3.6398e-13],\n",
       "       [ 3.1237e-05,  4.4527e-14],\n",
       "       [ 3.1237e-05,  8.0038e-14],\n",
       "       [ 3.1237e-05, -2.5578e-13],\n",
       "       [ 3.1237e-05,  4.0374e-13],\n",
       "       [ 3.1237e-05, -2.9595e-13],\n",
       "       [ 3.1237e-05,  1.3585e-13],\n",
       "       [ 3.1237e-05, -1.0755e-13],\n",
       "       [ 3.1237e-05,  2.9604e-13],\n",
       "       [ 3.1237e-05,  7.6689e-14],\n",
       "       [ 3.1237e-05,  4.1047e-13],\n",
       "       [ 3.1237e-05,  2.2103e-13],\n",
       "       [ 3.1237e-05,  -2.914e-13],\n",
       "       [ 3.1237e-05, -1.5076e-13],\n",
       "       [ 3.1237e-05,  4.5661e-13],\n",
       "       [ 3.1237e-05, -3.2837e-13],\n",
       "       [ 3.1237e-05, -2.6829e-13],\n",
       "       [ 3.1237e-05,  4.8286e-13],\n",
       "       [ 3.1237e-05,   1.916e-13],\n",
       "       [ 3.1237e-05, -3.9893e-13],\n",
       "       [ 3.1237e-05, -2.9514e-13],\n",
       "       [ 3.1237e-05, -3.5915e-14],\n",
       "       [ 3.1237e-05,  4.2189e-13],\n",
       "       [ 3.1237e-05, -8.0673e-14],\n",
       "       [ 3.1237e-05,  1.7171e-14],\n",
       "       [ 3.1237e-05,  1.0285e-13],\n",
       "       [ 3.1237e-05, -7.0016e-14],\n",
       "       [ 3.1237e-05, -3.9802e-14],\n",
       "       [ 3.1237e-05,  3.3638e-13],\n",
       "       [ 3.1237e-05, -3.7248e-13],\n",
       "       [ 3.1237e-05,  3.3468e-13],\n",
       "       [ 3.1237e-05, -3.9794e-13],\n",
       "       [ 3.1237e-05,  3.6298e-13],\n",
       "       [ 3.1237e-05,  -7.833e-14]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "reduced_features = sklearn.decomposition.PCA(n_components=2, svd_solver='arpack').fit_transform(feats_target.cpu().numpy())\n",
    "reduced_features\n",
    "#reduced_features = sklearn.manifold.TSNE(n_components=2).fit_transform(features[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def scale_to_01_range(x):\n",
    "    # compute the distribution range\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    " \n",
    "    # move the distribution so that it starts from zero\n",
    "    # by extracting the minimal value from all its values\n",
    "    starts_from_zero = x - np.min(x)\n",
    " \n",
    "    # make the distribution fit [0; 1] by dividing by its range\n",
    "    return starts_from_zero / value_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0.1412,     0.03765,           0,     0.94543,     0.69512,      0.3989,    0.069382,      0.5183,     0.55733,     0.18829,     0.91305,     0.14414,     0.61866,     0.35118,      0.7947,     0.55365,     0.92045,     0.71227,     0.14915,      0.3037,     0.97115,     0.10852,     0.17454,           1,\n",
       "           0.67993,    0.030979,     0.14504,      0.4299,     0.93299,     0.38072,     0.48824,     0.58239,     0.39243,     0.42563,     0.83903,    0.060046,     0.83715,    0.032066,     0.86825,     0.38329], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = scale_to_01_range(reduced_features[:,0])\n",
    "tx\n",
    "ty = scale_to_01_range(reduced_features[:,1])\n",
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    " \n",
    "colors_per_class = {0:'blue', 1:'red'}\n",
    "\n",
    "labels = []\n",
    "for img in selected_img:\n",
    "    labels.append(1 if os.path.basename(img).startswith('global_birds_palmyra') else 0)\n",
    "\n",
    "# for every class, we'll add a scatter plot separately\n",
    "for label in colors_per_class:\n",
    "    # find the samples of the current class in the data\n",
    "    indices = [i for i, l in enumerate(labels) if l == label]\n",
    " \n",
    "    # extract the coordinates of the points of this class only\n",
    "    current_tx = np.take(tx, indices)\n",
    "    current_ty = np.take(ty, indices)\n",
    " \n",
    "    # convert the class color to matplotlib format\n",
    "    color = colors_per_class[label] #np.array(colors_per_class[label], dtype=np.float) / 255\n",
    " \n",
    "    # add a scatter plot with the corresponding color and label\n",
    "    ax.scatter(current_tx, current_ty, c=color, label=label)\n",
    " \n",
    "# build a legend using the labels we set previously\n",
    "ax.legend(loc='best')\n",
    " \n",
    "# finally, show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'manifold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#reduced_features = sklearn.decomposition.PCA(n_components=2).fit_transform(feats_source.cpu().numpy())\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanifold\u001b[49m\u001b[38;5;241m.\u001b[39mTSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(feats_source\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      5\u001b[0m reduced_features\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'manifold'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "#reduced_features = sklearn.decomposition.PCA(n_components=2).fit_transform(feats_source.cpu().numpy())\n",
    "reduced_features = sklearn.manifold.TSNE(n_components=2).fit_transform(feats_source.cpu().numpy())\n",
    "reduced_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model\n",
    "for i, a in enumerate(model.model.model):\n",
    "    if i == 23:\n",
    "        print(\"block\", i)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsupervisedfeaturesdistance'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mB\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/runs'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_PATH = module_path + '/detect/YOLO_pe_palmyra_10percentbkgd_test2/weights/best.pt'\n",
    "model = YOLO('yolov8m.yaml', task='detect').load(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_img = []\n",
    "img_path = '/gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/test/'\n",
    "for subdataset in ['global_birds_penguins', 'global_birds_palmyra']:\n",
    "    selected_img.extend(random.choices(os.listdir(img_path + subdataset + '/images/'), k=10))\n",
    "\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = 0.1, \n",
    "        iou = 0.1,\n",
    "        show = False,\n",
    "        save = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[5].boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'deepcoral_detect' #detect\n",
    "#model_name = 'deepcoral_test4'\n",
    "#model_path = 'runs/' + TASK + '/' + model_name + '/weights/best.pt'\n",
    "#model = YOLO(model_path, TASK)\n",
    "model_name = 'deepcoral_background_lscale16_epochs20_coralgain10'\n",
    "PRETRAINED_MODEL_PATH = 'runs/' + TASK + '/' + model_name + '/weights/best.pt'\n",
    "\n",
    "model = YOLO(PRETRAINED_MODEL_PATH, TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['source', 'target'] #['global_birds_poland', 'global_birds_palmyra', 'global_birds_penguins', 'global_birds_pfeifer']\n",
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NORMAL MODEL\n",
    "# Select randomly 10 images from the test dataset\n",
    "\n",
    "if TASK == 'detect':\n",
    "    selected_img = []\n",
    "    for subdataset in datasets:\n",
    "        selected_img.extend(random.choices(os.listdir(img_path + subdataset + '/images/'), k=6))\n",
    "\n",
    "    results = model.predict(\n",
    "            #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "            source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "            conf = 0.2, \n",
    "            iou = 0.1,\n",
    "            show=False,\n",
    "            save=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'detect':\n",
    "    for img, result in zip(selected_img, results):\n",
    "\n",
    "        detection_boxes = []\n",
    "        save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "        for detect in range(len(result.boxes.cls)):\n",
    "            det = {}\n",
    "            det['conf'] = result.boxes.conf[detect].cpu()\n",
    "            det['category'] = result.boxes.cls[detect].cpu()\n",
    "            coords = result.boxes.xywhn[detect].cpu()\n",
    "            det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "            detection_boxes.append(det)\n",
    "            \n",
    "        im_path = os.path.join(img_path + 'images/', img)\n",
    "        visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                        confidence_threshold=0.0, detector_label_map=None,\n",
    "                                        thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "        selected_label = img_path + 'labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "        if os.path.exists(selected_label):\n",
    "            detection_boxes = []\n",
    "            df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "            for irow, row in df.iterrows():  \n",
    "                det = {}\n",
    "                det['conf'] = None\n",
    "                det['category'] = row[0]\n",
    "                det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "                detection_boxes.append(det)\n",
    "        \n",
    "            # Draw annotations\n",
    "            save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "            visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                            confidence_threshold=0.0, detector_label_map=None,\n",
    "                                            thickness=1,expansion=0, colormap=['SpringGreen'])\n",
    "                                            \n",
    "            # Remove predictions-only images\n",
    "            os.remove(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEEP CORAL MODEL\n",
    "# Select randomly 10 images from the test dataset\n",
    "\n",
    "if TASK == 'deepcoral_detect':\n",
    "\n",
    "    for subdataset in datasets:\n",
    "        selected_img = random.choices(os.listdir(img_path + subdataset + '/images/'), k=12)\n",
    "\n",
    "        results = model.predict(\n",
    "                #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "                source = [os.path.join(img_path, subdataset + '/images/', img) for img in selected_img],\n",
    "                conf = 0.2, \n",
    "                iou = 0.1,\n",
    "                show=False,\n",
    "                save=False\n",
    "            )\n",
    "        \n",
    "        for img, result in zip(selected_img, results):\n",
    "\n",
    "            detection_boxes = []\n",
    "            save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/' + TASK + '/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "            for detect in range(len(result.boxes.cls)):\n",
    "                det = {}\n",
    "                det['conf'] = result.boxes.conf[detect].cpu()\n",
    "                det['category'] = result.boxes.cls[detect].cpu()\n",
    "                coords = result.boxes.xywhn[detect].cpu()\n",
    "                det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "                detection_boxes.append(det)\n",
    "                \n",
    "            im_path = os.path.join(img_path + subdataset + '/images/', img)\n",
    "            visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                            confidence_threshold=0.0, detector_label_map=None,\n",
    "                                            thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "            selected_label = img_path  + subdataset + '/labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "            if os.path.exists(selected_label):\n",
    "                detection_boxes = []\n",
    "                df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "                for irow, row in df.iterrows():  \n",
    "                    det = {}\n",
    "                    det['conf'] = None\n",
    "                    det['category'] = row[0]\n",
    "                    det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "                    detection_boxes.append(det)\n",
    "        \n",
    "                # Draw annotations\n",
    "                save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/' + TASK + '/' + model_name + '/prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "                visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                                confidence_threshold=0.0, detector_label_map=None,\n",
    "                                                thickness=1,expansion=0, colormap=['SpringGreen'])\n",
    "                \n",
    "                # Remove predictions-only images\n",
    "                os.remove(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE the model's performance on the test set\n",
    "metrics = model.val(split='test', save_json=True, iou=0.1, conf=0.2, max_det=600)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "#success = model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.box.map)\n",
    "print(metrics.box.map50)    # map50-95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Evaluation per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =  ['global_birds_poland', 'global_birds_palmyra', 'global_birds_penguins',\n",
    "                    'global_birds_mckellar', 'global_birds_newmexico', \n",
    "                    'global_birds_pfeifer', 'uav_thermal_waterfowl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_penguins_poland_palmyra_mckellar_yolov8m_120epoch'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "\n",
    "    # Change test folder\n",
    "    fname = \"data.yaml\"\n",
    "    stream = open(fname, 'r')\n",
    "    data = yaml.safe_load(stream)\n",
    "    data['test'] = 'test/' + dataset + '/images/'\n",
    "    with open(fname, 'w') as yaml_file:\n",
    "        yaml_file.write( yaml.dump(data, default_flow_style=False))\n",
    "    \n",
    "    metrics = model.val(split='test', save_json=True, iou=0.1, max_det=600)\n",
    "    print(metrics.box.map50)\n",
    "\n",
    "# Change test folder to original name\n",
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "data['test'] = 'test/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_yolov8m_120epoch_default_batch32_aug90deg0.5flips_patience50_lr00.001_lrf0.0001'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'\n",
    "\n",
    "selected_img = (random.choices(os.listdir(img_path + '/images/'), k=1))\n",
    "selected_img = [os.path.join(img_path + '/images/', x) for x in selected_img]\n",
    "selected_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(selected_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
