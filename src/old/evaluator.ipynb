{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.199 ðŸš€ Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "Setup complete âœ… (32 CPUs, 251.6 GB RAM, 0.7/125.8 GB disk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.grace/eec42/BirdDetector/src\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/src/data_preprocessing\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "import random\n",
    "from ultralytics.utils.plotting import plot_labels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/data_preprocessing'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import visualization_utils as visutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0) # Set to your desired GPU number\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_penguins_poland_yolov8m_120epoch'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')\n",
    "\n",
    "IOU_THRESHOLD = 0.1\n",
    "CONF_THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_birds_pfeifer_Fregata_Island_2016_Chinstrap_penguins_205_patch_0.0_0.0_448_448.jpg']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'\n",
    "img_path\n",
    "\n",
    "selected_img = random.choices(os.listdir(img_path + '/images/'), k=1)\n",
    "selected_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 19 birds, 6.0ms\n",
      "Speed: 18.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'bird'}\n",
       " orig_img: array([[[67, 69, 69],\n",
       "         [66, 68, 68],\n",
       "         [66, 68, 68],\n",
       "         ...,\n",
       "         [87, 92, 91],\n",
       "         [84, 89, 87],\n",
       "         [73, 78, 76]],\n",
       " \n",
       "        [[66, 68, 68],\n",
       "         [65, 67, 67],\n",
       "         [66, 68, 68],\n",
       "         ...,\n",
       "         [77, 82, 81],\n",
       "         [79, 84, 82],\n",
       "         [74, 79, 77]],\n",
       " \n",
       "        [[68, 70, 70],\n",
       "         [68, 70, 70],\n",
       "         [70, 72, 72],\n",
       "         ...,\n",
       "         [67, 72, 71],\n",
       "         [71, 76, 74],\n",
       "         [72, 77, 75]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[66, 68, 68],\n",
       "         [62, 64, 64],\n",
       "         [57, 59, 59],\n",
       "         ...,\n",
       "         [47, 52, 53],\n",
       "         [43, 48, 49],\n",
       "         [40, 45, 46]],\n",
       " \n",
       "        [[67, 69, 69],\n",
       "         [64, 66, 66],\n",
       "         [59, 61, 61],\n",
       "         ...,\n",
       "         [50, 55, 56],\n",
       "         [47, 52, 53],\n",
       "         [44, 49, 50]],\n",
       " \n",
       "        [[65, 67, 67],\n",
       "         [63, 65, 65],\n",
       "         [60, 62, 62],\n",
       "         ...,\n",
       "         [54, 59, 60],\n",
       "         [52, 57, 58],\n",
       "         [49, 54, 55]]], dtype=uint8)\n",
       " orig_shape: (448, 448)\n",
       " path: '/gpfs/gibbs/project/jetz/eec42/data/baseline1_pfeifer_penguins_poland_no_background/test/images/global_birds_pfeifer_Fregata_Island_2016_Chinstrap_penguins_205_patch_0.0_0.0_448_448.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 17.984628677368164, 'inference': 6.000518798828125, 'postprocess': 1.0170936584472656}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions with right iou & confidence threshold\n",
    "\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = CONF_THRESHOLD, \n",
    "        iou = IOU_THRESHOLD,\n",
    "        show=False,\n",
    "        save=False\n",
    "    )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detections:  19\n",
      "Number of detection labels:  17\n"
     ]
    }
   ],
   "source": [
    "result = results[0]\n",
    "img = selected_img[0]\n",
    "\n",
    "detection_boxes = []\n",
    "save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "for detect in range(len(result.boxes.cls)):\n",
    "    det = {}\n",
    "    det['conf'] = result.boxes.conf[detect].cpu()\n",
    "    det['category'] = result.boxes.cls[detect].cpu()\n",
    "    coords = result.boxes.xywhn[detect].cpu()\n",
    "    det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "    detection_boxes.append(det)\n",
    "\n",
    "print(\"Number of detections: \", len(detection_boxes))\n",
    "\n",
    "im_path = os.path.join(img_path + 'images/', img)\n",
    "visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                confidence_threshold=0.0, detector_label_map=None,\n",
    "                                thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "selected_label = img_path + 'labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "detection_boxes = []\n",
    "true_bboxes = torch.tensor([], dtype=torch.float32)\n",
    "true_classes = torch.tensor([], dtype=torch.float32)\n",
    "df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "for irow, row in df.iterrows():  \n",
    "    det = {}\n",
    "    det['conf'] = None\n",
    "    det['category'] = row[0]\n",
    "    true_classes = torch.cat((true_classes, torch.tensor([row[0]], dtype=torch.float32)), 0)\n",
    "    det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "    true_bboxes = torch.cat((true_bboxes, torch.tensor([[row[1]-row[3]/2, row[2]-row[4]/2, row[1]+row[3]/2, row[2]+row[4]/2]], dtype=torch.float32)), 0)\n",
    "\n",
    "    detection_boxes.append(det)\n",
    "\n",
    "# Draw annotations\n",
    "print(\"Number of detection labels: \", len(detection_boxes))\n",
    "save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/EVALUATOR_TEST_prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                confidence_threshold=0.0, detector_label_map=None,\n",
    "                                thickness=1,expansion=0, colormap=['SpringGreen'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match detections with ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "\n",
    "    Args:\n",
    "        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n",
    "        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n",
    "    \"\"\"\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n",
    "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp_(0).prod(2)\n",
    "\n",
    "    # IoU = inter / (area1 + area2 - inter)\n",
    "    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = result.boxes.cls\n",
    "pred_classes = pred_classes.cpu()\n",
    "pred_bboxes = result.boxes.xyxyn\n",
    "pred_bboxes = pred_bboxes.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3661e-01, 1.5245e-01, 7.9937e-01, 2.1814e-01],\n",
       "        [7.7830e-01, 3.1854e-02, 8.4100e-01, 9.7088e-02],\n",
       "        [0.0000e+00, 7.1415e-02, 5.6110e-02, 1.3782e-01],\n",
       "        [4.6395e-01, 3.2652e-03, 5.2825e-01, 6.9059e-02],\n",
       "        [2.6355e-01, 0.0000e+00, 3.3269e-01, 3.1120e-02],\n",
       "        [6.9119e-01, 1.2916e-03, 7.5584e-01, 6.7873e-02],\n",
       "        [6.7284e-01, 1.4715e-01, 7.3572e-01, 2.1079e-01],\n",
       "        [2.2937e-01, 1.6163e-01, 2.9279e-01, 2.2360e-01],\n",
       "        [6.8913e-01, 8.1183e-02, 7.5224e-01, 1.4439e-01],\n",
       "        [5.4456e-01, 3.5860e-02, 6.0829e-01, 1.0114e-01],\n",
       "        [8.1954e-01, 1.0968e-01, 8.8081e-01, 1.7149e-01],\n",
       "        [5.9723e-01, 1.1590e-04, 6.6117e-01, 3.7605e-02],\n",
       "        [5.8292e-02, 9.2950e-02, 1.2168e-01, 1.5734e-01],\n",
       "        [2.8510e-02, 0.0000e+00, 9.8326e-02, 5.1345e-02],\n",
       "        [1.0550e-01, 1.9162e-02, 1.6885e-01, 8.5249e-02],\n",
       "        [9.2208e-01, 3.3882e-01, 9.8723e-01, 4.0320e-01],\n",
       "        [8.4570e-01, 4.1182e-02, 9.0683e-01, 1.0141e-01],\n",
       "        [3.6091e-01, 0.0000e+00, 4.2952e-01, 2.9070e-02],\n",
       "        [8.3465e-01, 2.1639e-01, 8.9425e-01, 2.7613e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  8.2589e-02,  5.5804e-02,  1.3839e-01],\n",
       "        [ 2.9018e-02, -4.8572e-17,  8.4821e-02,  5.1339e-02],\n",
       "        [ 1.0938e-01,  2.9018e-02,  1.6518e-01,  8.4821e-02],\n",
       "        [ 6.4732e-02,  8.9286e-02,  1.2054e-01,  1.4509e-01],\n",
       "        [ 4.6652e-01,  1.7857e-02,  5.2232e-01,  7.3661e-02],\n",
       "        [ 5.6027e-01,  5.3571e-02,  6.1607e-01,  1.0938e-01],\n",
       "        [ 6.9643e-01,  8.9286e-03,  7.5223e-01,  6.4732e-02],\n",
       "        [ 6.8750e-01,  8.4821e-02,  7.4330e-01,  1.4062e-01],\n",
       "        [ 6.7634e-01,  1.4955e-01,  7.3214e-01,  2.0536e-01],\n",
       "        [ 7.4107e-01,  1.5625e-01,  7.9688e-01,  2.1205e-01],\n",
       "        [ 7.6562e-01,  1.2277e-01,  8.2143e-01,  1.7857e-01],\n",
       "        [ 7.8125e-01,  2.9018e-02,  8.3705e-01,  8.4821e-02],\n",
       "        [ 8.3929e-01,  4.0179e-02,  8.9509e-01,  9.5982e-02],\n",
       "        [ 8.2366e-01,  1.1161e-01,  8.7946e-01,  1.6741e-01],\n",
       "        [ 8.4598e-01,  2.2098e-01,  9.0179e-01,  2.7679e-01],\n",
       "        [ 9.3304e-01,  3.4375e-01,  9.8884e-01,  3.9955e-01],\n",
       "        [ 8.8839e-01,  3.6161e-01,  9.4420e-01,  4.1741e-01]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = box_iou(true_bboxes, pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8201, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7992, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7437, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6788, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.6368, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4578, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7409, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7782, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7553, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1387, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.6957, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6618, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8223, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6632],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7059, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1440, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/eec42/BirdDetector/src/model/evaluator.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Br903u37n01/home/eec42/BirdDetector/src/model/evaluator.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#pred_bboxes = result.boxes\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Br903u37n01/home/eec42/BirdDetector/src/model/evaluator.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_bboxes\u001b[39m.\u001b[39;49mxyxy[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Br903u37n01/home/eec42/BirdDetector/src/model/evaluator.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_bboxes\u001b[39m.\u001b[39mxywh[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Br903u37n01/home/eec42/BirdDetector/src/model/evaluator.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_bboxes\u001b[39m.\u001b[39mxywh[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m pred_bboxes\u001b[39m.\u001b[39mxywh[\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'xyxy'"
     ]
    }
   ],
   "source": [
    "#pred_bboxes = result.boxes\n",
    "print(pred_bboxes.xyxy[0])\n",
    "print(pred_bboxes.xywh[0])\n",
    "\n",
    "print(pred_bboxes.xywh[0][0] - pred_bboxes.xywh[0][2]/2)\n",
    "print(pred_bboxes.xyxy[0][0])\n",
    "print(pred_bboxes.xywh[0][0] - pred_bboxes.xywh[0][2]/2 == pred_bboxes.xyxy[0][0])\n",
    "print(pred_bboxes.xywh[0][0] + pred_bboxes.xywh[0][2]/2 == pred_bboxes.xyxy[0][2])\n",
    "print(pred_bboxes.xywh[0][1] - pred_bboxes.xywh[0][3]/2)\n",
    "print(pred_bboxes.xyxy[0][3])\n",
    "print(pred_bboxes.xywh[0][1] - pred_bboxes.xywh[0][3]/2 == pred_bboxes.xyxy[0][1])\n",
    "print(pred_bboxes.xywh[0][1] + pred_bboxes.xywh[0][3]/2 == pred_bboxes.xyxy[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_predictions method from YOLOv8 code - try to reuse it to simplify\n",
    "\n",
    "def match_predictions(pred_classes, true_classes, iou, use_scipy=False):\n",
    "    \"\"\"\n",
    "    Matches predictions to ground truth objects (pred_classes, true_classes) using IoU.\n",
    "\n",
    "    Args:\n",
    "        pred_classes (torch.Tensor): Predicted class indices of shape(N,).\n",
    "        true_classes (torch.Tensor): Target class indices of shape(M,).\n",
    "        iou (torch.Tensor): An NxM tensor containing the pairwise IoU values for predictions and ground of truth\n",
    "        use_scipy (bool): Whether to use scipy for matching (more precise).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Correct tensor of shape(N,1) for 1 IoU thresholds.\n",
    "    \"\"\"\n",
    "    # Dx10 matrix, where D - detections, 10 - IoU thresholds\n",
    "    correct = np.zeros((pred_classes.shape[0], 1)).astype(bool)\n",
    "    # LxD matrix where L - labels (rows), D - detections (columns)\n",
    "    correct_class = true_classes[:, None] == pred_classes\n",
    "    iou = iou * correct_class  # zero out the wrong classes\n",
    "    iou = iou.cpu().numpy()\n",
    "    threshold = IOU_THRESHOLD\n",
    "    #for i, threshold in enumerate(self.iouv.cpu().tolist()):\n",
    "    if use_scipy:\n",
    "        # WARNING: known issue that reduces mAP in https://github.com/ultralytics/ultralytics/pull/4708\n",
    "        import scipy  # scope import to avoid importing for all commands\n",
    "        cost_matrix = iou * (iou >= threshold)\n",
    "        if cost_matrix.any():\n",
    "            labels_idx, detections_idx = scipy.optimize.linear_sum_assignment(cost_matrix, maximize=True)\n",
    "            valid = cost_matrix[labels_idx, detections_idx] > 0\n",
    "            if valid.any():\n",
    "                correct[detections_idx[valid]] = True\n",
    "    else:\n",
    "        matches = np.nonzero(iou >= threshold)  # IoU > threshold and classes match\n",
    "        matches = np.array(matches).T\n",
    "        if matches.shape[0]:\n",
    "            if matches.shape[0] > 1:\n",
    "                matches = matches[iou[matches[:, 0], matches[:, 1]].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "                # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "            correct[matches[:, 1].astype(int)] = True\n",
    "    return torch.tensor(correct, dtype=torch.bool, device=pred_classes.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = match_predictions(pred_classes, true_classes, iou)  # what they call tp in the code !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15) tensor(2) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "TP = correct.sum()\n",
    "FN = len(true_bboxes) - TP\n",
    "FP = len(pred_bboxes) - TP\n",
    "print(TP, FN, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recall curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print precision curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_per_class(tp,\n",
    "                 conf,\n",
    "                 pred_cls,\n",
    "                 target_cls,\n",
    "                 plot=False,\n",
    "                 on_plot=None,\n",
    "                 save_dir=Path(),\n",
    "                 names=(),\n",
    "                 eps=1e-16,\n",
    "                 prefix=''):\n",
    "    \"\"\"\n",
    "    Computes the average precision per class for object detection evaluation.\n",
    "\n",
    "    Args:\n",
    "        tp (np.ndarray): Binary array indicating whether the detection is correct (True) or not (False).\n",
    "        conf (np.ndarray): Array of confidence scores of the detections.\n",
    "        pred_cls (np.ndarray): Array of predicted classes of the detections.\n",
    "        target_cls (np.ndarray): Array of true classes of the detections.\n",
    "        plot (bool, optional): Whether to plot PR curves or not. Defaults to False.\n",
    "        on_plot (func, optional): A callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        save_dir (Path, optional): Directory to save the PR curves. Defaults to an empty path.\n",
    "        names (tuple, optional): Tuple of class names to plot PR curves. Defaults to an empty tuple.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-16.\n",
    "        prefix (str, optional): A prefix string for saving the plot files. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple of six arrays and one array of unique classes, where:\n",
    "            tp (np.ndarray): True positive counts at threshold given by max F1 metric for each class.Shape: (nc,).\n",
    "            fp (np.ndarray): False positive counts at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            p (np.ndarray): Precision values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            r (np.ndarray): Recall values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            f1 (np.ndarray): F1-score values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            ap (np.ndarray): Average precision for each class at different IoU thresholds. Shape: (nc, 10).\n",
    "            unique_classes (np.ndarray): An array of unique classes that have data. Shape: (nc,).\n",
    "            p_curve (np.ndarray): Precision curves for each class. Shape: (nc, 1000).\n",
    "            r_curve (np.ndarray): Recall curves for each class. Shape: (nc, 1000).\n",
    "            f1_curve (np.ndarray): F1-score curves for each class. Shape: (nc, 1000).\n",
    "            x (np.ndarray): X-axis values for the curves. Shape: (1000,).\n",
    "            prec_values: Precision values at mAP@0.5 for each class. Shape: (nc, 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    x, prec_values = np.linspace(0, 1, 1000), []\n",
    "\n",
    "    # Average precision, precision and recall curves\n",
    "    ap, p_curve, r_curve = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (n_l + eps)  # recall curve\n",
    "        r_curve[ci] = np.interp(-x, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p_curve[ci] = np.interp(-x, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "            if plot and j == 0:\n",
    "                prec_values.append(np.interp(x, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    prec_values = np.array(prec_values)  # (nc, 1000)\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1_curve = 2 * p_curve * r_curve / (p_curve + r_curve + eps)\n",
    "    names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "    names = dict(enumerate(names))  # to dict\n",
    "    if plot:\n",
    "        plot_pr_curve(x, prec_values, ap, save_dir / f'{prefix}PR_curve.png', names, on_plot=on_plot)\n",
    "        #plot_mc_curve(x, f1_curve, save_dir / f'{prefix}F1_curve.png', names, ylabel='F1', on_plot=on_plot)\n",
    "        #plot_mc_curve(x, p_curve, save_dir / f'{prefix}P_curve.png', names, ylabel='Precision', on_plot=on_plot)\n",
    "        #plot_mc_curve(x, r_curve, save_dir / f'{prefix}R_curve.png', names, ylabel='Recall', on_plot=on_plot)\n",
    "\n",
    "    i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p_curve[:, i], r_curve[:, i], f1_curve[:, i]  # max-F1 precision, recall, F1 values\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    return tp, fp, p, r, f1, ap, unique_classes.astype(int), p_curve, r_curve, f1_curve, x, prec_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(recall, precision):\n",
    "    \"\"\"\n",
    "    Compute the average precision (AP) given the recall and precision curves.\n",
    "\n",
    "    Args:\n",
    "        recall (list): The recall curve.\n",
    "        precision (list): The precision curve.\n",
    "\n",
    "    Returns:\n",
    "        (float): Average precision.\n",
    "        (np.ndarray): Precision envelope curve.\n",
    "        (np.ndarray): Modified recall curve with sentinel values added at the beginning and end.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([1.0], precision, [0.0]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
    "\n",
    "    # Integrate area under curve\n",
    "    method = 'interp'  # methods: 'continuous', 'interp'\n",
    "    if method == 'interp':\n",
    "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
    "    else:  # 'continuous'\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x-axis (recall) changes\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n",
    "\n",
    "    return ap, mpre, mrec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
