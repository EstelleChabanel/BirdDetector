{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.0 🚀 Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "Setup complete ✅ (32 CPUs, 251.6 GB RAM, 1.4/125.8 GB disk)\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/src\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/src/data_preprocessing\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#import dayolo\n",
    "#from dayolo import YOLO\n",
    "\n",
    "#import yolo\n",
    "#from yolo import YOLO\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "import random\n",
    "#from ultralytics.utils.plotting import plot_labels\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/data_preprocessing'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "#import visualization_utils as visutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0) # Set to your desired GPU number\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING Multi CLASSIFIER MODEL\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 22                   4  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 23                  -1  1    129603  yolo.nn.modules.dan.Conv_                    [192, 64, 32]                 \n",
      " 24                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 25                   6  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 26                  -1  1    517251  yolo.nn.modules.dan.Conv_                    [384, 128, 64]                \n",
      " 27                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 28                   9  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 29                  -1  1   1624323  yolo.nn.modules.dan.Conv_                    [576, 256, 128]               \n",
      " 30                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 31                   9  1         0  yolo.nn.modules.dan.AvgPooling               [1]                           \n",
      " 32        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "YOLOv8m summary: 340 layers, 28128088 parameters, 28128072 gradients\n",
      "\n",
      "Transferred 390/544 items from pretrained weights\n",
      "Transferred 390/544 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "# Create a new YOLO model from scratch\n",
    "#model = YOLO('yolov8n.yaml')\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training) - for now, we keep the small version\n",
    "#pretrained_model_name = 'pfeifer_penguins_poland_10percentbckgd_yolov8m_120epoch'\n",
    "#model_path = 'runs/detect/' + pretrained_model_name + '/weights/best.pt'\n",
    "#model = YOLO('yolov8m.pt', task='da_detect')\n",
    "#model = YOLO(\"yolov8.yaml\").load(\"yolov8m.pt\")\n",
    "#model = YOLO('yolov8m.pt', task='da_detect')\n",
    "#model = YOLO('yolov8m.pt', task='detect')\n",
    "#model = YOLO(\"yolov8m.yaml\", task='detect').load(\"yolov8m.pt\")\n",
    "model = YOLO(\"yolov8m.yaml\", task='detect', subtask='multidomainclassifier').load(\"yolov8m.pt\")\n",
    "\n",
    "\n",
    "#model.load_weights('yolov8m.pt')\n",
    "\n",
    "\n",
    "#PRETRAINED_MODEL_NAME = 'pfeifer_penguins_poland_10percentbckgd_yolov8m_120epoch'\n",
    "#PRETRAINED_MODEL_PATH = 'src/model/runs/detect/' + PRETRAINED_MODEL_NAME + '/weights/best.pt'\n",
    "\n",
    "#MODEL_NAME = 'pfeifer_penguins_poland_palmyra_10percent_bckgd_yolov8m_120epochs'\n",
    "#TASK = 'detect' # Choose between: 'deepcoral_detect' 'detect'\n",
    "#model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect\n",
      "multidomainclassifier\n"
     ]
    }
   ],
   "source": [
    "print(model.task)\n",
    "print(model.subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 23\n",
      "Conv_(\n",
      "  (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(1, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.model.model\n",
    "for i, a in enumerate(model.model.model):\n",
    "    if i == 23:\n",
    "        print(\"block\", i)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.4 available 😃 Update with 'pip install -U ultralytics'\n",
      "New https://pypi.org/project/ultralytics/8.1.4 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.228 🚀 Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "Ultralytics YOLOv8.0.228 🚀 Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=data.yaml, epochs=2, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=TEST_features_loss2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, source_name=global_birds_palmyra, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, dc=1.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=90, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/TEST_features_loss2\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=data.yaml, epochs=2, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=TEST_features_loss2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, source_name=global_birds_palmyra, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, dc=1.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=90, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/TEST_features_loss2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING Multi CLASSIFIER MODEL\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 22                   4  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 22                   4  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 23                  -1  1    129603  yolo.nn.modules.dan.Conv_                    [192, 64, 32]                 \n",
      " 23                  -1  1    129603  yolo.nn.modules.dan.Conv_                    [192, 64, 32]                 \n",
      " 24                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 24                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 25                   6  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 25                   6  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 26                  -1  1    517251  yolo.nn.modules.dan.Conv_                    [384, 128, 64]                \n",
      " 26                  -1  1    517251  yolo.nn.modules.dan.Conv_                    [384, 128, 64]                \n",
      " 27                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 27                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 28                   9  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 28                   9  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 29                  -1  1   1624323  yolo.nn.modules.dan.Conv_                    [576, 256, 128]               \n",
      " 29                  -1  1   1624323  yolo.nn.modules.dan.Conv_                    [576, 256, 128]               \n",
      " 30                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 30                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 31                   9  1         0  yolo.nn.modules.dan.AvgPooling               [1]                           \n",
      " 31                   9  1         0  yolo.nn.modules.dan.AvgPooling               [1]                           \n",
      " 32        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      " 32        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "after Adapt block torch.Size([1, 2])\n",
      "YOLOv8m summary: 340 layers, 28128088 parameters, 28128072 gradients\n",
      "YOLOv8m summary: 340 layers, 28128088 parameters, 28128072 gradients\n",
      "\n",
      "\n",
      "Transferred 544/544 items from pretrained weights\n",
      "Transferred 544/544 items from pretrained weights\n",
      "Freezing layer 'model.32.dfl.conv.weight'\n",
      "Freezing layer 'model.32.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/train/labels.cache... 1722 images, 182 backgrounds, 0 corrupt: 100%|██████████| 1904/1904 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/val/labels.cache... 248 images, 19 backgrounds, 0 corrupt: 100%|██████████| 267/267 [00:00<?, ?it/s]\n",
      "/home/eec42/.conda/envs/pdm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/TEST_features_loss2/labels.jpg... \n",
      "Plotting labels to runs/detect/TEST_features_loss2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 86 weight(decay=0.0), 96 weight(decay=0.0005), 104 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 86 weight(decay=0.0), 96 weight(decay=0.0005), 104 bias(decay=0.0)\n",
      "2 epochs...\n",
      "2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  da_loss_s  da_loss_m  da_loss_l  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  da_loss_s  da_loss_m  da_loss_l  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      5.06G      6.653      25.21      4.195      0.705     0.6165     0.9631         27        640:   0%|          | 1/238 [00:01<04:46,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      5.06G      6.662      32.46      4.295     0.6952     0.6441     0.8643         90        640:   1%|          | 2/238 [00:01<02:38,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      5.06G      6.714      31.06        4.3     0.7101     0.6883      0.794         74        640:   2%|▏         | 4/238 [00:02<01:34,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      5.06G      6.612      27.62      4.301     0.7019     0.6832     0.8097         27        640:   2%|▏         | 5/238 [00:02<01:33,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      5.06G      6.679      25.21      4.297     0.6937     0.6946     0.7878         52        640:   3%|▎         | 6/238 [00:02<01:20,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      6.75G      6.553      21.31      4.296     0.6949     0.6659     0.8328         79        640:   3%|▎         | 8/238 [00:03<01:08,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      6.75G      6.563      24.93      4.289     0.6894     0.6502     0.8479          5        640:   4%|▍         | 10/238 [00:03<01:04,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      6.75G      6.572      22.94      4.285     0.6865     0.6456     0.8581         67        640:   5%|▌         | 12/238 [00:04<01:03,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      6.75G      6.548      20.95      4.277     0.6844     0.6358     0.8727         37        640:   6%|▌         | 14/238 [00:05<01:01,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      6.75G      6.568      20.89      4.256     0.6827     0.6309     0.8697         16        640:   7%|▋         | 16/238 [00:05<01:01,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      6.75G      6.568      20.89      4.256     0.6827     0.6309     0.8697         16        640:   7%|▋         | 16/238 [00:05<01:18,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after Adapt block torch.Size([8, 2])\n",
      "after Adapt block torch.Size([8, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TRAIN the model on our dataset (data.yml config file) \u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST_features_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#imgsz=480,  # we are trying with several img size so we do not precise the size -> will automatically resize all images to 640x640\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m   \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#32,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#cos_lr=True,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#dropout=0.3,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#optimizer='Adam',\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m   \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m   \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#lr0=0.001,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#lrf=0.0001,\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfliplr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflipud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# augmentation parameters\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m   \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/engine/model.py:340\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/engine/trainer.py:190\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/engine/trainer.py:342\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    341\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/.conda/envs/pdm/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pdm/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/nn/tasks.py:43\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/nn/tasks.py:218\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    217\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vast/palmer/home.grace/eec42/BirdDetector/yolo/yolo/utils/loss.py:418\u001b[0m, in \u001b[0;36mv8MultiDomainClassifierLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    416\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    417\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 418\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[1;32m    419\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN the model on our dataset (data.yml config file) \n",
    "\n",
    "model_name = 'TEST_features_loss'\n",
    "\n",
    "results = model.train(\n",
    "   data='data.yaml',\n",
    "   #imgsz=480,  # we are trying with several img size so we do not precise the size -> will automatically resize all images to 640x640\n",
    "   epochs=2,\n",
    "   batch=8, #32,\n",
    "   #cos_lr=True,\n",
    "   #dropout=0.3,\n",
    "   #optimizer='Adam',\n",
    "   patience=10,\n",
    "   device=0,\n",
    "   verbose=True,\n",
    "   val = True,\n",
    "   #lr0=0.001,\n",
    "   #lrf=0.0001,\n",
    "   degrees=90, fliplr=0.5, flipud=0.5, scale=0.5, # augmentation parameters\n",
    "   name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_img = []\n",
    "img_path = '/gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/test/'\n",
    "selected_img.extend(random.choices(os.listdir(img_path + '/images/'), k=5))\n",
    "\n",
    "# Predict results for randomly selected images\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = 0.1, \n",
    "        iou = 0.1,\n",
    "        show = False,\n",
    "        save = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model\n",
    "for i, a in enumerate(model.model.model):\n",
    "    if i == 23:\n",
    "        print(\"block\", i)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.grace/eec42/BirdDetector\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/runs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/runs'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3822016  ultralytics.nn.modules.head.Detect           [80, [192, 384, 576]]         \n",
      "YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients, 79.3 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH = module_path + '/detect/YOLO_pe_palmyra_10percentbkgd_test2/weights/best.pt'\n",
    "model = YOLO('yolov8m.yaml', task='detect').load(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 1: 640x640 (no detections), 2: 640x640 (no detections), 3: 640x640 (no detections), 4: 640x640 (no detections), 5: 640x640 (no detections), 6: 640x640 (no detections), 7: 640x640 (no detections), 8: 640x640 (no detections), 9: 640x640 (no detections), 10: 640x640 (no detections), 11: 640x640 (no detections), 12: 640x640 (no detections), 13: 640x640 (no detections), 14: 640x640 (no detections), 15: 640x640 (no detections), 16: 640x640 (no detections), 17: 640x640 (no detections), 18: 640x640 (no detections), 19: 640x640 (no detections), 196.3ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "selected_img = []\n",
    "img_path = '/gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/test/'\n",
    "for subdataset in ['global_birds_penguins', 'global_birds_palmyra']:\n",
    "    selected_img.extend(random.choices(os.listdir(img_path + subdataset + '/images/'), k=10))\n",
    "\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = 0.1, \n",
    "        iou = 0.1,\n",
    "        show = False,\n",
    "        save = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([], device='cuda:0')\n",
       "conf: tensor([], device='cuda:0')\n",
       "data: tensor([], device='cuda:0', size=(0, 6))\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (480, 480)\n",
       "shape: torch.Size([0, 6])\n",
       "xywh: tensor([], device='cuda:0', size=(0, 4))\n",
       "xywhn: tensor([], device='cuda:0', size=(0, 4))\n",
       "xyxy: tensor([], device='cuda:0', size=(0, 4))\n",
       "xyxyn: tensor([], device='cuda:0', size=(0, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[5].boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'deepcoral_detect' #detect\n",
    "#model_name = 'deepcoral_test4'\n",
    "#model_path = 'runs/' + TASK + '/' + model_name + '/weights/best.pt'\n",
    "#model = YOLO(model_path, TASK)\n",
    "model_name = 'deepcoral_background_lscale16_epochs20_coralgain10'\n",
    "PRETRAINED_MODEL_PATH = 'runs/' + TASK + '/' + model_name + '/weights/best.pt'\n",
    "\n",
    "model = YOLO(PRETRAINED_MODEL_PATH, TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['source', 'target'] #['global_birds_poland', 'global_birds_palmyra', 'global_birds_penguins', 'global_birds_pfeifer']\n",
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NORMAL MODEL\n",
    "# Select randomly 10 images from the test dataset\n",
    "\n",
    "if TASK == 'detect':\n",
    "    selected_img = []\n",
    "    for subdataset in datasets:\n",
    "        selected_img.extend(random.choices(os.listdir(img_path + subdataset + '/images/'), k=6))\n",
    "\n",
    "    results = model.predict(\n",
    "            #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "            source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "            conf = 0.2, \n",
    "            iou = 0.1,\n",
    "            show=False,\n",
    "            save=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'detect':\n",
    "    for img, result in zip(selected_img, results):\n",
    "\n",
    "        detection_boxes = []\n",
    "        save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "        for detect in range(len(result.boxes.cls)):\n",
    "            det = {}\n",
    "            det['conf'] = result.boxes.conf[detect].cpu()\n",
    "            det['category'] = result.boxes.cls[detect].cpu()\n",
    "            coords = result.boxes.xywhn[detect].cpu()\n",
    "            det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "            detection_boxes.append(det)\n",
    "            \n",
    "        im_path = os.path.join(img_path + 'images/', img)\n",
    "        visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                        confidence_threshold=0.0, detector_label_map=None,\n",
    "                                        thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "        selected_label = img_path + 'labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "        if os.path.exists(selected_label):\n",
    "            detection_boxes = []\n",
    "            df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "            for irow, row in df.iterrows():  \n",
    "                det = {}\n",
    "                det['conf'] = None\n",
    "                det['category'] = row[0]\n",
    "                det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "                detection_boxes.append(det)\n",
    "        \n",
    "            # Draw annotations\n",
    "            save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "            visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                            confidence_threshold=0.0, detector_label_map=None,\n",
    "                                            thickness=1,expansion=0, colormap=['SpringGreen'])\n",
    "                                            \n",
    "            # Remove predictions-only images\n",
    "            os.remove(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEEP CORAL MODEL\n",
    "# Select randomly 10 images from the test dataset\n",
    "\n",
    "if TASK == 'deepcoral_detect':\n",
    "\n",
    "    for subdataset in datasets:\n",
    "        selected_img = random.choices(os.listdir(img_path + subdataset + '/images/'), k=12)\n",
    "\n",
    "        results = model.predict(\n",
    "                #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "                source = [os.path.join(img_path, subdataset + '/images/', img) for img in selected_img],\n",
    "                conf = 0.2, \n",
    "                iou = 0.1,\n",
    "                show=False,\n",
    "                save=False\n",
    "            )\n",
    "        \n",
    "        for img, result in zip(selected_img, results):\n",
    "\n",
    "            detection_boxes = []\n",
    "            save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/' + TASK + '/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "            for detect in range(len(result.boxes.cls)):\n",
    "                det = {}\n",
    "                det['conf'] = result.boxes.conf[detect].cpu()\n",
    "                det['category'] = result.boxes.cls[detect].cpu()\n",
    "                coords = result.boxes.xywhn[detect].cpu()\n",
    "                det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "                detection_boxes.append(det)\n",
    "                \n",
    "            im_path = os.path.join(img_path + subdataset + '/images/', img)\n",
    "            visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                            confidence_threshold=0.0, detector_label_map=None,\n",
    "                                            thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "            selected_label = img_path  + subdataset + '/labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "            if os.path.exists(selected_label):\n",
    "                detection_boxes = []\n",
    "                df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "                for irow, row in df.iterrows():  \n",
    "                    det = {}\n",
    "                    det['conf'] = None\n",
    "                    det['category'] = row[0]\n",
    "                    det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "                    detection_boxes.append(det)\n",
    "        \n",
    "                # Draw annotations\n",
    "                save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/' + TASK + '/' + model_name + '/prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "                visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                                confidence_threshold=0.0, detector_label_map=None,\n",
    "                                                thickness=1,expansion=0, colormap=['SpringGreen'])\n",
    "                \n",
    "                # Remove predictions-only images\n",
    "                os.remove(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE the model's performance on the test set\n",
    "metrics = model.val(split='test', save_json=True, iou=0.1, conf=0.2, max_det=600)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "#success = model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.box.map)\n",
    "print(metrics.box.map50)    # map50-95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Evaluation per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =  ['global_birds_poland', 'global_birds_palmyra', 'global_birds_penguins',\n",
    "                    'global_birds_mckellar', 'global_birds_newmexico', \n",
    "                    'global_birds_pfeifer', 'uav_thermal_waterfowl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_penguins_poland_palmyra_mckellar_yolov8m_120epoch'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "\n",
    "    # Change test folder\n",
    "    fname = \"data.yaml\"\n",
    "    stream = open(fname, 'r')\n",
    "    data = yaml.safe_load(stream)\n",
    "    data['test'] = 'test/' + dataset + '/images/'\n",
    "    with open(fname, 'w') as yaml_file:\n",
    "        yaml_file.write( yaml.dump(data, default_flow_style=False))\n",
    "    \n",
    "    metrics = model.val(split='test', save_json=True, iou=0.1, max_det=600)\n",
    "    print(metrics.box.map50)\n",
    "\n",
    "# Change test folder to original name\n",
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "data['test'] = 'test/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_yolov8m_120epoch_default_batch32_aug90deg0.5flips_patience50_lr00.001_lrf0.0001'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'\n",
    "\n",
    "selected_img = (random.choices(os.listdir(img_path + '/images/'), k=1))\n",
    "selected_img = [os.path.join(img_path + '/images/', x) for x in selected_img]\n",
    "selected_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(selected_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
