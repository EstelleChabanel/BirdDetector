{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.grace/eec42/BirdDetector/src\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/src/data_preprocessing\n"
     ]
    }
   ],
   "source": [
    "#import ultralytics\n",
    "#ultralytics.checks()\n",
    "#from ultralytics import YOLO\n",
    "\n",
    "#import dayolo\n",
    "#from dayolo import YOLO\n",
    "\n",
    "import yolo\n",
    "from yolo import YOLO\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "import random\n",
    "#from ultralytics.utils.plotting import plot_labels\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/data_preprocessing'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "#import visualization_utils as visutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0) # Set to your desired GPU number\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING DOMAIN CLASSIFIER MODEL\n",
      "INITIALIZING DOMAIN CLASSIFIER MODEL\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  yolo.nn.modules.conv.Conv                    [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  yolo.nn.modules.conv.Conv                    [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  yolo.nn.modules.block.C2f                    [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  yolo.nn.modules.conv.Conv                    [32, 64, 3, 2]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4                  -1  2     49664  yolo.nn.modules.block.C2f                    [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  yolo.nn.modules.conv.Conv                    [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  yolo.nn.modules.block.C2f                    [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  yolo.nn.modules.conv.Conv                    [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  yolo.nn.modules.block.C2f                    [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  yolo.nn.modules.block.SPPF                   [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  1    148224  yolo.nn.modules.block.C2f                    [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  1     37248  yolo.nn.modules.block.C2f                    [192, 64, 1]                  \n",
      " 16                  -1  1     36992  yolo.nn.modules.conv.Conv                    [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  1    123648  yolo.nn.modules.block.C2f                    [192, 128, 1]                 \n",
      " 19                  -1  1    147712  yolo.nn.modules.conv.Conv                    [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  1    493056  yolo.nn.modules.block.C2f                    [384, 256, 1]                 \n",
      " 50                   9  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 51                  -1  1    887043  yolo.nn.modules.dan.Conv_                    [256, 256, 128]               \n",
      " 52                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 54        [15, 18, 21]  1    751507  yolo.nn.modules.head.Detect                  [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 239 layers, 3898090 parameters, 3898074 gradients\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 54        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      "YOLOv8m summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Transferred 469/475 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "# Create a new YOLO model from scratch\n",
    "#model = YOLO('yolov8n.yaml')\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training) - for now, we keep the small version\n",
    "#pretrained_model_name = 'pfeifer_penguins_poland_10percentbckgd_yolov8m_120epoch'\n",
    "#model_path = 'runs/detect/' + pretrained_model_name + '/weights/best.pt'\n",
    "#model = YOLO('yolov8m.pt', task='da_detect')\n",
    "#model = YOLO(\"yolov8.yaml\").load(\"yolov8m.pt\")\n",
    "#model = YOLO('yolov8m.pt', task='da_detect')\n",
    "#model = YOLO('yolov8m.pt', task='detect')\n",
    "#model = YOLO(\"yolov8m.yaml\", task='detect').load(\"yolov8m.pt\")\n",
    "model = YOLO(\"yolov8m.yaml\", task='detect', subtask='unsuperviseddomainclassifier').load(\"yolov8m.pt\")\n",
    "\n",
    "\n",
    "#model.load_weights('yolov8m.pt')\n",
    "\n",
    "\n",
    "#PRETRAINED_MODEL_NAME = 'pfeifer_penguins_poland_10percentbckgd_yolov8m_120epoch'\n",
    "#PRETRAINED_MODEL_PATH = 'src/model/runs/detect/' + PRETRAINED_MODEL_NAME + '/weights/best.pt'\n",
    "\n",
    "#MODEL_NAME = 'pfeifer_penguins_poland_palmyra_10percent_bckgd_yolov8m_120epochs'\n",
    "#TASK = 'detect' # Choose between: 'deepcoral_detect' 'detect'\n",
    "#model = YOLO('yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect\n",
      "unsuperviseddomainclassifier\n"
     ]
    }
   ],
   "source": [
    "print(model.task)\n",
    "print(model.subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model\n",
    "for i, a in enumerate(model.model.model):\n",
    "    if i == 24:\n",
    "        print(\"block\", i)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.6 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "New https://pypi.org/project/ultralytics/8.1.6 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.228 ðŸš€ Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "Ultralytics YOLOv8.0.228 ðŸš€ Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=data.yaml, epochs=2, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=TEST_features_loss4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, source_name=global_birds_palmyra, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, dc=1.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=90, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/TEST_features_loss4\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=data.yaml, epochs=2, time=None, patience=10, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=TEST_features_loss4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, source_name=global_birds_palmyra, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, dc=1.0, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=90, translate=0.0, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/TEST_features_loss4\n",
      "INITIALIZING DOMAIN CLASSIFIER MODEL\n",
      "INITIALIZING DOMAIN CLASSIFIER MODEL\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  yolo.nn.modules.conv.Conv                    [3, 16, 3, 2]                 \n",
      "  0                  -1  1       464  yolo.nn.modules.conv.Conv                    [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  yolo.nn.modules.conv.Conv                    [16, 32, 3, 2]                \n",
      "  1                  -1  1      4672  yolo.nn.modules.conv.Conv                    [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  yolo.nn.modules.block.C2f                    [32, 32, 1, True]             \n",
      "  2                  -1  1      7360  yolo.nn.modules.block.C2f                    [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  yolo.nn.modules.conv.Conv                    [32, 64, 3, 2]                \n",
      "  3                  -1  1     18560  yolo.nn.modules.conv.Conv                    [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  yolo.nn.modules.block.C2f                    [64, 64, 2, True]             \n",
      "  4                  -1  2     49664  yolo.nn.modules.block.C2f                    [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  yolo.nn.modules.conv.Conv                    [64, 128, 3, 2]               \n",
      "  5                  -1  1     73984  yolo.nn.modules.conv.Conv                    [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  yolo.nn.modules.block.C2f                    [128, 128, 2, True]           \n",
      "  6                  -1  2    197632  yolo.nn.modules.block.C2f                    [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  yolo.nn.modules.conv.Conv                    [128, 256, 3, 2]              \n",
      "  7                  -1  1    295424  yolo.nn.modules.conv.Conv                    [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  yolo.nn.modules.block.C2f                    [256, 256, 1, True]           \n",
      "  8                  -1  1    460288  yolo.nn.modules.block.C2f                    [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  yolo.nn.modules.block.SPPF                   [256, 256, 5]                 \n",
      "  9                  -1  1    164608  yolo.nn.modules.block.SPPF                   [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  1    148224  yolo.nn.modules.block.C2f                    [384, 128, 1]                 \n",
      " 12                  -1  1    148224  yolo.nn.modules.block.C2f                    [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  1     37248  yolo.nn.modules.block.C2f                    [192, 64, 1]                  \n",
      " 15                  -1  1     37248  yolo.nn.modules.block.C2f                    [192, 64, 1]                  \n",
      " 16                  -1  1     36992  yolo.nn.modules.conv.Conv                    [64, 64, 3, 2]                \n",
      " 16                  -1  1     36992  yolo.nn.modules.conv.Conv                    [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  1    123648  yolo.nn.modules.block.C2f                    [192, 128, 1]                 \n",
      " 18                  -1  1    123648  yolo.nn.modules.block.C2f                    [192, 128, 1]                 \n",
      " 19                  -1  1    147712  yolo.nn.modules.conv.Conv                    [128, 128, 3, 2]              \n",
      " 19                  -1  1    147712  yolo.nn.modules.conv.Conv                    [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  1    493056  yolo.nn.modules.block.C2f                    [384, 256, 1]                 \n",
      " 21                  -1  1    493056  yolo.nn.modules.block.C2f                    [384, 256, 1]                 \n",
      " 50                   9  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 50                   9  1         0  yolo.nn.modules.dan.GradReversal             [1]                           \n",
      " 51                  -1  1    887043  yolo.nn.modules.dan.Conv_                    [256, 256, 128]               \n",
      " 51                  -1  1    887043  yolo.nn.modules.dan.Conv_                    [256, 256, 128]               \n",
      " 52                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 52                  -1  1         4  yolo.nn.modules.dan.AdaptiveAvgPooling       [1]                           \n",
      " 54        [15, 18, 21]  1    751507  yolo.nn.modules.head.Detect                  [1, [64, 128, 256]]           \n",
      " 54        [15, 18, 21]  1    751507  yolo.nn.modules.head.Detect                  [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 239 layers, 3898090 parameters, 3898074 gradients\n",
      "YOLOv8n summary: 239 layers, 3898090 parameters, 3898074 gradients\n",
      "\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  0                  -1  1      1392  yolo.nn.modules.conv.Conv                    [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  1                  -1  1     41664  yolo.nn.modules.conv.Conv                    [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  2                  -1  2    111360  yolo.nn.modules.block.C2f                    [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  3                  -1  1    166272  yolo.nn.modules.conv.Conv                    [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  4                  -1  4    813312  yolo.nn.modules.block.C2f                    [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  5                  -1  1    664320  yolo.nn.modules.conv.Conv                    [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  6                  -1  4   3248640  yolo.nn.modules.block.C2f                    [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  7                  -1  1   1991808  yolo.nn.modules.conv.Conv                    [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  8                  -1  2   3985920  yolo.nn.modules.block.C2f                    [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      "  9                  -1  1    831168  yolo.nn.modules.block.SPPF                   [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 11             [-1, 6]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 12                  -1  2   1993728  yolo.nn.modules.block.C2f                    [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 14             [-1, 4]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 15                  -1  2    517632  yolo.nn.modules.block.C2f                    [576, 192, 2]                 \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 16                  -1  1    332160  yolo.nn.modules.conv.Conv                    [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 17            [-1, 12]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 18                  -1  2   1846272  yolo.nn.modules.block.C2f                    [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 19                  -1  1   1327872  yolo.nn.modules.conv.Conv                    [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 20             [-1, 9]  1         0  yolo.nn.modules.conv.Concat                  [1]                           \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 21                  -1  2   4207104  yolo.nn.modules.block.C2f                    [960, 576, 2]                 \n",
      " 54        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      " 54        [15, 18, 21]  1   3776275  yolo.nn.modules.head.Detect                  [1, [192, 384, 576]]          \n",
      "YOLOv8m summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "YOLOv8m summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "Transferred 475/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_10percent_background_unsupervised/train/source/labels.cache... 626 images, 62 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_10percent_background_unsupervised/train/target/train/labels.cache... 1096 images, 104 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /gpfs/gibbs/project/jetz/eec42/data/pe_10percent_background_unsupervised/val/labels.cache... 82 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:00<?, ?it/s]\n",
      "/home/eec42/.conda/envs/pdm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/TEST_features_loss4/labels.jpg... \n",
      "Plotting labels to runs/detect/TEST_features_loss4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "2 epochs...\n",
      "2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss    da_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss    da_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         84       1351       0.29      0.292      0.192     0.0322\n",
      "                   all         84       1351       0.29      0.292      0.192     0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        1/2      9.75G      2.473      1.956      1.322     0.7212         85        640:   0%|          | 0/150 [00:18<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss    da_loss  Instances       Size\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss    da_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         84       1351      0.587      0.437      0.414     0.0807\n",
      "                   all         84       1351      0.587      0.437      0.414     0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.010 hours.\n",
      "\n",
      "2 epochs completed in 0.010 hours.\n",
      "Optimizer stripped from runs/detect/TEST_features_loss4/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/TEST_features_loss4/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/TEST_features_loss4/weights/best.pt, 52.0MB\n",
      "Optimizer stripped from runs/detect/TEST_features_loss4/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating runs/detect/TEST_features_loss4/weights/best.pt...\n",
      "\n",
      "Validating runs/detect/TEST_features_loss4/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.228 ðŸš€ Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "Ultralytics YOLOv8.0.228 ðŸš€ Python-3.11.6 torch-2.1.0+cu121 CUDA:0 (NVIDIA RTX A5000, 24248MiB)\n",
      "YOLOv8m summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "YOLOv8m summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         84       1351      0.582      0.436      0.413     0.0807\n",
      "                   all         84       1351      0.582      0.436      0.413     0.0807\n",
      "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/TEST_features_loss4\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/TEST_features_loss4\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2      9.71G      2.262       1.55      1.208     0.7214         84        640:   0%|          | 0/150 [00:22<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN the model on our dataset (data.yml config file) \n",
    "\n",
    "model_name = 'TEST_features_loss'\n",
    "\n",
    "results = model.train(\n",
    "   data='data.yaml',\n",
    "   #imgsz=480,  # we are trying with several img size so we do not precise the size -> will automatically resize all images to 640x640\n",
    "   epochs=2,\n",
    "   batch=8, #32,\n",
    "   #cos_lr=True,\n",
    "   #dropout=0.3,\n",
    "   #optimizer='Adam',\n",
    "   patience=10,\n",
    "   device=0,\n",
    "   verbose=True,\n",
    "   val = True,\n",
    "   #lr0=0.001,\n",
    "   #lrf=0.0001,\n",
    "   degrees=90, fliplr=0.5, flipud=0.5, scale=0.5, # augmentation parameters\n",
    "   name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx = torch.Tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 3., 3., 4., 5., 6., 7.])\n",
    "annotations_mask = [False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False]\n",
    "\n",
    "new_batch_idx = batch_idx[annotations_mask]\n",
    "new_batch_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = new_batch_idx.unique(return_inverse=True)\n",
    "b[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0: 640x640 (no detections), 1: 640x640 102 birds, 2: 640x640 2 birds, 3: 640x640 2 birds, 4: 640x640 (no detections), 110.7ms\n",
      "0: 640x640 (no detections), 1: 640x640 102 birds, 2: 640x640 2 birds, 3: 640x640 2 birds, 4: 640x640 (no detections), 110.7ms\n",
      "Speed: 1.3ms preprocess, 22.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Speed: 1.3ms preprocess, 22.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_img = []\n",
    "img_path = '/gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/test/'\n",
    "selected_img.extend(random.choices(os.listdir(img_path + '/images/'), k=5))\n",
    "\n",
    "# Predict results for randomly selected images\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = 0.1, \n",
    "        iou = 0.1,\n",
    "        show = False,\n",
    "        save = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model\n",
    "for i, a in enumerate(model.model.model):\n",
    "    if i == 23:\n",
    "        print(\"block\", i)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsuperviseddomainclassifier'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mB\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/palmer/home.grace/eec42/BirdDetector\n",
      "/vast/palmer/home.grace/eec42/BirdDetector/runs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..'))\n",
    "print(module_path)\n",
    "module_path = module_path+'/runs'\n",
    "print(module_path)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3822016  ultralytics.nn.modules.head.Detect           [80, [192, 384, 576]]         \n",
      "YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients, 79.3 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH = module_path + '/detect/YOLO_pe_palmyra_10percentbkgd_test2/weights/best.pt'\n",
    "model = YOLO('yolov8m.yaml', task='detect').load(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 1: 640x640 (no detections), 2: 640x640 (no detections), 3: 640x640 (no detections), 4: 640x640 (no detections), 5: 640x640 (no detections), 6: 640x640 (no detections), 7: 640x640 (no detections), 8: 640x640 (no detections), 9: 640x640 (no detections), 10: 640x640 (no detections), 11: 640x640 (no detections), 12: 640x640 (no detections), 13: 640x640 (no detections), 14: 640x640 (no detections), 15: 640x640 (no detections), 16: 640x640 (no detections), 17: 640x640 (no detections), 18: 640x640 (no detections), 19: 640x640 (no detections), 196.3ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "selected_img = []\n",
    "img_path = '/gpfs/gibbs/project/jetz/eec42/data/pe_palmyra_10percentbkgd/test/'\n",
    "for subdataset in ['global_birds_penguins', 'global_birds_palmyra']:\n",
    "    selected_img.extend(random.choices(os.listdir(img_path + subdataset + '/images/'), k=10))\n",
    "\n",
    "results = model.predict(\n",
    "        #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "        source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "        conf = 0.1, \n",
    "        iou = 0.1,\n",
    "        show = False,\n",
    "        save = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([], device='cuda:0')\n",
       "conf: tensor([], device='cuda:0')\n",
       "data: tensor([], device='cuda:0', size=(0, 6))\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (480, 480)\n",
       "shape: torch.Size([0, 6])\n",
       "xywh: tensor([], device='cuda:0', size=(0, 4))\n",
       "xywhn: tensor([], device='cuda:0', size=(0, 4))\n",
       "xyxy: tensor([], device='cuda:0', size=(0, 4))\n",
       "xyxyn: tensor([], device='cuda:0', size=(0, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[5].boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'deepcoral_detect' #detect\n",
    "#model_name = 'deepcoral_test4'\n",
    "#model_path = 'runs/' + TASK + '/' + model_name + '/weights/best.pt'\n",
    "#model = YOLO(model_path, TASK)\n",
    "model_name = 'deepcoral_background_lscale16_epochs20_coralgain10'\n",
    "PRETRAINED_MODEL_PATH = 'runs/' + TASK + '/' + model_name + '/weights/best.pt'\n",
    "\n",
    "model = YOLO(PRETRAINED_MODEL_PATH, TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['source', 'target'] #['global_birds_poland', 'global_birds_palmyra', 'global_birds_penguins', 'global_birds_pfeifer']\n",
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR NORMAL MODEL\n",
    "# Select randomly 10 images from the test dataset\n",
    "\n",
    "if TASK == 'detect':\n",
    "    selected_img = []\n",
    "    for subdataset in datasets:\n",
    "        selected_img.extend(random.choices(os.listdir(img_path + subdataset + '/images/'), k=6))\n",
    "\n",
    "    results = model.predict(\n",
    "            #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "            source = [os.path.join(img_path + 'images/', img) for img in selected_img],\n",
    "            conf = 0.2, \n",
    "            iou = 0.1,\n",
    "            show=False,\n",
    "            save=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'detect':\n",
    "    for img, result in zip(selected_img, results):\n",
    "\n",
    "        detection_boxes = []\n",
    "        save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "        for detect in range(len(result.boxes.cls)):\n",
    "            det = {}\n",
    "            det['conf'] = result.boxes.conf[detect].cpu()\n",
    "            det['category'] = result.boxes.cls[detect].cpu()\n",
    "            coords = result.boxes.xywhn[detect].cpu()\n",
    "            det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "            detection_boxes.append(det)\n",
    "            \n",
    "        im_path = os.path.join(img_path + 'images/', img)\n",
    "        visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                        confidence_threshold=0.0, detector_label_map=None,\n",
    "                                        thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "        selected_label = img_path + 'labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "        if os.path.exists(selected_label):\n",
    "            detection_boxes = []\n",
    "            df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "            for irow, row in df.iterrows():  \n",
    "                det = {}\n",
    "                det['conf'] = None\n",
    "                det['category'] = row[0]\n",
    "                det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "                detection_boxes.append(det)\n",
    "        \n",
    "            # Draw annotations\n",
    "            save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/detect/' + model_name + '/prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "            visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                            confidence_threshold=0.0, detector_label_map=None,\n",
    "                                            thickness=1,expansion=0, colormap=['SpringGreen'])\n",
    "                                            \n",
    "            # Remove predictions-only images\n",
    "            os.remove(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEEP CORAL MODEL\n",
    "# Select randomly 10 images from the test dataset\n",
    "\n",
    "if TASK == 'deepcoral_detect':\n",
    "\n",
    "    for subdataset in datasets:\n",
    "        selected_img = random.choices(os.listdir(img_path + subdataset + '/images/'), k=12)\n",
    "\n",
    "        results = model.predict(\n",
    "                #model = 'runs/detect/pfeifer_yolov8n_70epoch_default_batch32_dropout0.3',\n",
    "                source = [os.path.join(img_path, subdataset + '/images/', img) for img in selected_img],\n",
    "                conf = 0.2, \n",
    "                iou = 0.1,\n",
    "                show=False,\n",
    "                save=False\n",
    "            )\n",
    "        \n",
    "        for img, result in zip(selected_img, results):\n",
    "\n",
    "            detection_boxes = []\n",
    "            save_path = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/' + TASK + '/' + model_name + '/prediction_' + os.path.basename(result.path).split('.jpg')[0] + '.jpg'\n",
    "            for detect in range(len(result.boxes.cls)):\n",
    "                det = {}\n",
    "                det['conf'] = result.boxes.conf[detect].cpu()\n",
    "                det['category'] = result.boxes.cls[detect].cpu()\n",
    "                coords = result.boxes.xywhn[detect].cpu()\n",
    "                det['bbox'] = [coords[0]-coords[2]/2, coords[1]-coords[3]/2, coords[2], coords[3]]\n",
    "                detection_boxes.append(det)\n",
    "                \n",
    "            im_path = os.path.join(img_path + subdataset + '/images/', img)\n",
    "            visutils.draw_bounding_boxes_on_file(im_path, save_path, detection_boxes,\n",
    "                                            confidence_threshold=0.0, detector_label_map=None,\n",
    "                                            thickness=1,expansion=0, colormap=['Red'])\n",
    "\n",
    "            selected_label = img_path  + subdataset + '/labels/' + os.path.basename(result.path).split('.jpg')[0] + '.txt'\n",
    "            if os.path.exists(selected_label):\n",
    "                detection_boxes = []\n",
    "                df = pd.read_csv(selected_label, sep='\\t', header=None, index_col=False)\n",
    "                for irow, row in df.iterrows():  \n",
    "                    det = {}\n",
    "                    det['conf'] = None\n",
    "                    det['category'] = row[0]\n",
    "                    det['bbox'] = [row[1]-row[3]/2, row[2]-row[4]/2, row[3], row[4]]\n",
    "                    detection_boxes.append(det)\n",
    "        \n",
    "                # Draw annotations\n",
    "                save_path2 = '/vast/palmer/home.grace/eec42/BirdDetector/src/model/runs/' + TASK + '/' + model_name + '/prediction_label_' + os.path.basename(result.path).split('.hpg')[0] + '.jpg'\n",
    "                visutils.draw_bounding_boxes_on_file(save_path, save_path2, detection_boxes,\n",
    "                                                confidence_threshold=0.0, detector_label_map=None,\n",
    "                                                thickness=1,expansion=0, colormap=['SpringGreen'])\n",
    "                \n",
    "                # Remove predictions-only images\n",
    "                os.remove(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE the model's performance on the test set\n",
    "metrics = model.val(split='test', save_json=True, iou=0.1, conf=0.2, max_det=600)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "#success = model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.box.map)\n",
    "print(metrics.box.map50)    # map50-95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Evaluation per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =  ['global_birds_poland', 'global_birds_palmyra', 'global_birds_penguins',\n",
    "                    'global_birds_mckellar', 'global_birds_newmexico', \n",
    "                    'global_birds_pfeifer', 'uav_thermal_waterfowl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_penguins_poland_palmyra_mckellar_yolov8m_120epoch'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "\n",
    "    # Change test folder\n",
    "    fname = \"data.yaml\"\n",
    "    stream = open(fname, 'r')\n",
    "    data = yaml.safe_load(stream)\n",
    "    data['test'] = 'test/' + dataset + '/images/'\n",
    "    with open(fname, 'w') as yaml_file:\n",
    "        yaml_file.write( yaml.dump(data, default_flow_style=False))\n",
    "    \n",
    "    metrics = model.val(split='test', save_json=True, iou=0.1, max_det=600)\n",
    "    print(metrics.box.map50)\n",
    "\n",
    "# Change test folder to original name\n",
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "data['test'] = 'test/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pfeifer_yolov8m_120epoch_default_batch32_aug90deg0.5flips_patience50_lr00.001_lrf0.0001'\n",
    "model = YOLO('runs/detect/' + model_name + '/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data.yaml\"\n",
    "stream = open(fname, 'r')\n",
    "data = yaml.safe_load(stream)\n",
    "img_path = data['path'] + '/test/'\n",
    "\n",
    "selected_img = (random.choices(os.listdir(img_path + '/images/'), k=1))\n",
    "selected_img = [os.path.join(img_path + '/images/', x) for x in selected_img]\n",
    "selected_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(selected_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
